# lg-agent-v1.py
## code generated by ChatGPT 4o 
'''
The prompt used to generate this code:
use langgraph as the agent framework for the following Agent capabilities:
- read local arxiv pub PDF files
- create and apply meaningful tags to help categorize papers
- generate a markdown list of papers. include for each paper:
    - file shortcut to the PDF (all papers are on a NAS on the local private network) 
    - a brief one sentence distillation of the abstract
    - relevant tags.
'''

import os
import pathlib
import openai
from pypdf import PdfReader
import langgraph.graph as lg
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI  # we'll configure this to point to your local endpoint

from typing import TypedDict, List

class PDFState(TypedDict):
    pdf_dir: str
    papers: List[dict]
    markdown: str

# Set up OpenAI client to talk to text-generation-webui local API
openai.api_key = "sk-xxx"  # dummy, not used
openai.api_base =  "http://127.0.0.1:5000/v1"  # adjust if needed
openai.api_type = "openai"

llm = ChatOpenAI(
    openai_api_base=openai.api_base,
    openai_api_key=openai.api_key,
    model="qwen2.5-3b-instruct-q8_0.gguf",  # or whatever model name webui reports
    temperature=0.3,
)

# -- Node 1: Read PDFs --
def read_pdfs_node(state):
    pdf_dir = state["pdf_dir"]
    pdf_files = list(pathlib.Path(pdf_dir).rglob("*.pdf"))

    ## list "folder1", "folder2" etc. to exclude
    exclude_paths = []
    

    papers = []
    items = 1
    for pdf_file in pdf_files:
        relative_path = str(pdf_file.relative_to(pdf_dir)).lower()

        # -- Exclude unwanted folders --
        if any(exclude in relative_path for exclude in exclude_paths):
            print(f"Skipping {relative_path} (excluded folder)")
            continue

        try:
            reader = PdfReader(str(pdf_file))
            ## We expect the first few pages to contain key info
            text = "\n".join(page.extract_text() for page in reader.pages[:3] if page.extract_text())
            papers.append({
                "file_path": str(pdf_file),
                "text": text
            })
            print(f"{items}: Extracted {len(text)} characters from {pdf_file}")
            items += 1
            # TEST RUN
            if items > 20:
                break
        except Exception as e:
            print(f"Skipping {relative_path} (could not read PDF: {e})")
            continue  # Skip corrupted or invalid PDFs

    state["papers"] = papers
    return state


# -- Node 2: Extract Abstract and Tags --
def analyze_papers_node(state):
    updated_papers = []
    for paper in state["papers"]:
        prompt = (
            "You are reading the first few pages of a scientific paper.\n"
            "Extract the following fields and respond ONLY as a JSON object with these exact keys:\n"
            "- title (string)\n"
            "- first_author (string)\n"
            "- abstract_summary (one sentence string)\n"
            "- tags (list of strings)\n\n"
            "Paper Text:\n"
            f"{paper['text'][:3000]}"
        )
        response = llm.invoke(prompt)
        paper["analysis"] = response.content
        updated_papers.append(paper)
    state["papers"] = updated_papers
    return state

# -- Node 3: Generate Markdown Listing using JSON keys --
def generate_markdown_node(state):
    md_lines = ["# Papers Summary\n"]
    for paper in state["papers"]:
        filename = pathlib.Path(paper["file_path"]).name
        shortcut = f"[{filename}]({paper['file_path']})"

        metadata = paper["metadata"]
        md_lines.append(f"## {metadata['title']} â€” {metadata['first_author']}\n")
        md_lines.append(f"**File:** {shortcut}\n")
        md_lines.append(f"**Summary:** {metadata['abstract_summary']}\n")
        md_lines.append(f"**Tags:** {', '.join(metadata['tags'])}\n")
        md_lines.append("\n")

    md_output = "\n".join(md_lines)
    state["markdown"] = md_output
    return state

# -- Build the graph --
graph = lg.StateGraph(PDFState)
graph.add_node("read_pdfs", read_pdfs_node)
graph.add_node("analyze_papers", analyze_papers_node)
graph.add_node("generate_markdown", generate_markdown_node)

graph.set_entry_point("read_pdfs")
graph.add_edge("read_pdfs", "analyze_papers")
graph.add_edge("analyze_papers", "generate_markdown")

graph.set_finish_point("generate_markdown")

compiled_graph = graph.compile()

# -- Run the agent --
if __name__ == "__main__":
    input_state = {
        "pdf_dir": "/mnt/athena/SynologyDrive/arxiv",  
    }
    final_state = compiled_graph.invoke(input_state)
    # Save the markdown output
    with open("papers_summary.md", "w") as f:
        f.write(final_state["markdown"])
    print("Markdown summary created!")
