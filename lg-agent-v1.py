# lg-agent-v1.py
## code generated by ChatGPT 4o 


import os
import pathlib
import openai
from pypdf import PdfReader
import langgraph.graph as lg
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI  # we'll configure this to point to your local endpoint

from typing import TypedDict, List

class PDFState(TypedDict):
    pdf_dir: str
    papers: List[dict]
    markdown: str

# Set up OpenAI client to talk to text-generation-webui local API
openai.api_key = "sk-xxx"  # dummy, not used
openai.api_base =  "http://127.0.0.1:5000/v1"  # adjust if needed
openai.api_type = "openai"

llm = ChatOpenAI(
    openai_api_base=openai.api_base,
    openai_api_key=openai.api_key,
    model="qwen2.5-3b-instruct-q8_0.gguf",  # or whatever model name webui reports
    temperature=0.3,
)

# -- Node 1: Read PDFs --
def read_pdfs_node(state):
    pdf_dir = state["pdf_dir"]
    pdf_files = list(pathlib.Path(pdf_dir).rglob("*.pdf"))

    exclude_paths = [
        "mit_courseware",
        "jason_brownlee_ebooks",
        "textbooks",
    ]

    papers = []
    items = 1
    for pdf_file in pdf_files:
        relative_path = str(pdf_file.relative_to(pdf_dir)).lower()

        # -- Exclude unwanted folders --
        if any(exclude in relative_path for exclude in exclude_paths):
            print(f"Skipping {relative_path} (excluded folder)")
            continue

        try:
            reader = PdfReader(str(pdf_file))
            text = "\n".join(page.extract_text() for page in reader.pages if page.extract_text())
            papers.append({
                "file_path": str(pdf_file),
                "text": text
            })
            print(f"{items}: Extracted {len(text)} characters from {pdf_file}")
            items += 1
            # TEST RUN
            if items > 5:
                break
        except Exception as e:
            print(f"Skipping {relative_path} (could not read PDF: {e})")
            continue  # Skip corrupted or invalid PDFs

    state["papers"] = papers
    return state


# -- Node 2: Extract Abstract and Tags --
def analyze_papers_node(state):
    updated_papers = []
    for paper in state["papers"]:
        prompt = (
            "Given the following academic paper content, extract:\n"
            "1. A one-sentence summary of the abstract.\n"
            "2. 3-5 meaningful tags categorizing the paper.\n\n"
            f"Paper Content:\n{paper['text'][:3000]}"  # limit text if needed
        )
        response = llm.invoke(prompt)
        paper["analysis"] = response.content
        updated_papers.append(paper)
    state["papers"] = updated_papers
    return state

# -- Node 3: Generate Markdown Listing --
def generate_markdown_node(state):
    md_lines = ["# Papers Summary\n"]
    for paper in state["papers"]:
        filename = pathlib.Path(paper["file_path"]).name
        shortcut = f"[{filename}]({paper['file_path']})"
        md_lines.append(f"## {shortcut}\n")
        md_lines.append(paper["analysis"])
        md_lines.append("\n")
    md_output = "\n".join(md_lines)
    state["markdown"] = md_output
    return state

# -- Build the graph --
graph = lg.StateGraph(PDFState)
graph.add_node("read_pdfs", read_pdfs_node)
graph.add_node("analyze_papers", analyze_papers_node)
graph.add_node("generate_markdown", generate_markdown_node)

graph.set_entry_point("read_pdfs")
graph.add_edge("read_pdfs", "analyze_papers")
graph.add_edge("analyze_papers", "generate_markdown")

graph.set_finish_point("generate_markdown")

compiled_graph = graph.compile()

# -- Run the agent --
if __name__ == "__main__":
    input_state = {
        "pdf_dir": "/mnt/c/Users/johna/OneDrive/Documents/literature/SynologyDrive",  
    }
    final_state = compiled_graph.invoke(input_state)
    # Save the markdown output
    with open("papers_summary.md", "w") as f:
        f.write(final_state["markdown"])
    print("Markdown summary created!")
